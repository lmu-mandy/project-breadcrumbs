# -*- coding: utf-8 -*-
"""NLP Baseline Model v1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P7IABjyyH1v660COZv5L7Ha7Vwn5lmoD

This notebook uses GPT-2 from simpletransformers to generate fairy tale text. For our baseline model, the model is not fine-tuned, but tested on fairy tale data from the Blue Fairy Book.
"""

!pip install simpletransformers

import torch
# If there's a GPU available...
if torch.cuda.is_available():    
    # Tell PyTorch to use the GPU.    
    device = torch.device('cuda')
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device('cpu')

"""Prepare the data!"""

import pandas as pd

df = pd.read_csv('bfb.txt', sep=r'\s{2,}', engine='python', header=None, names=['Line'])
lines = df['Line'].tolist()
train_cutoff = int(len(lines) * 0.8)

# Train data is the first 80% of the fairy tale data
with open('train.txt', 'w') as f:
   for line in lines[:-train_cutoff]:
       f.writelines(line + '\n')

# Testing data is the last 20% of the fairy tale data
with open('test.txt', 'w') as f:
   for line in lines[-train_cutoff:]:
       f.writelines(line + '\n')

"""Define GPT-2 baseline model (without fine-tuning on fairy tale training data) and evaluate model on fairy tale test data."""

from simpletransformers.language_modeling import LanguageModelingModel
import logging

logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

train_args = {
    'reprocess_input_data': True,
    'overwrite_output_dir': True,
    'train_batch_size': 32,
    'num_train_epochs': 5,
    'mlm': False,
}

# Create a baseline GPT-2 model that is not fine-tuned on fairy tale data.
baseline_model = LanguageModelingModel('gpt2', 'gpt2', args=train_args)

baseline_model.eval_model('test.txt')

"""Define GPT-2 model that is fine-tuned on fairy tales and evaluate it on fairy tale test data."""

# Create a GPT-2 model that is fine-tuned on fairy tale data.
finetuned_model = LanguageModelingModel('gpt2', 'gpt2', args=train_args)

finetuned_model.train_model('train.txt', eval_file='test.txt')

finetuned_model.eval_model('test.txt')

"""Generate fairy tale text from fine-tuned GPT-2 model."""

from simpletransformers.language_generation import LanguageGenerationModel

# Generate fairy tale on fine-tuned model.
generator = LanguageGenerationModel('gpt2', './outputs', args={'max_length': 256})

# Prompts based on fairy tales!
prompts = [
    "Once upon a time there was a young girl who was the fairest of them all. Her sisters were jealous of her beauty.",
    "Alone in the woods was a small girl in a red cape. She was on her way to bring bread to her grandmother.",
    "A long time ago, there was a couple who wanted more than anything to have a child of their own.",
    "Out in a cave, there lived a horrible beast. Nobody dared go near it, and the cave remained untouched by the nearby village for thousands of years.",
]

for prompt in prompts:
    generated = generator.generate(prompt, verbose=False)

    generated = '.'.join(generated[0].split('.')[:-1]) + '.'
    print('\n\n' + generated)

