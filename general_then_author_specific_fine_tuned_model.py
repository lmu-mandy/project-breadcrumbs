# -*- coding: utf-8 -*-
"""NLP Model Fine-tuned on Generic Fairy Tales and then on Specific Authors

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tusxZ4SKJY_sRhl4neXppIJ_i2NDEWf6

This notebook uses GPT-2 from simpletransformers to generate fairy tale text. 
For our general then author specific fine tuned model, the model is fine tuned on a general set of of fairy tales, excluding fairy tales from authors we will specifically use, then on the user-chosen author, and tested on fairy tale data from that specific author or a general fairy tale. 
Adjustments to training and testing data are up to user preference.
"""

!pip install simpletransformers

import torch
if torch.cuda.is_available():    
    device = torch.device('cuda')
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device('cpu')

"""Uncomment the line for the author that you want to generate a language model for!
(Note: Only one of the author lines should be uncommented)
"""

author = "grimms"
# author = "andersen"
# author = "perrault"

authors_to_data = {"grimms": 'grimms.txt', "andersen": 'andersen.txt', "perrault": 'perrault.txt'}
author_data = authors_to_data[author]

"""Preprocess general and author specific fairy tale data and split each of datasets into 80% training and 20% testing"""

import pandas as pd

# Concatenate author data (that is not from the chosen author) to our existing large fairy tale dataset

authors_to_data.pop(author)

with open('combined.txt', 'a') as outfile:
    for author in authors_to_data.values():
        with open(author) as infile:
            outfile.write(infile.read())

general_data = pd.read_csv('combined.txt', sep=r' - (?={)', engine='python', header=None, names=['Line'])
general_lines = general_data['Line'].tolist()
train_cutoff_general = int(len(general_lines) * 0.8)

# Training data is the first 80% of the fairy tale data, testing is the last 20%
with open('general_train.txt', 'w') as f:
   for line in general_lines[:-train_cutoff_general]:
       f.writelines(line + '\n')

with open('general_test.txt', 'w') as f:
   for line in general_lines[-train_cutoff_general:]:
       f.writelines(line + '\n')

# Author-specific data
author_data = pd.read_csv(author_data, sep=r' - (?={)', engine='python', header=None, names=['Line'])
author_lines = author_data['Line'].tolist()
train_cutoff_author = int(len(author_lines) * 0.8)

with open('author_specific_train.txt', 'w') as f:
  for line in author_lines[:-train_cutoff_author]:
    f.writelines(line + '\n') 

with open('author_specific_test.txt', 'w') as f:
  for line in author_lines[-train_cutoff_author:]:
    f.writelines(line + '\n')

"""Define GPT-2 model that is fine-tuned on general fairy tales then fine tuned
on specific author styles and evaluate it on the some test data.
"""

from simpletransformers.language_modeling import LanguageModelingModel
import logging

logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

train_args = {
    'reprocess_input_data': True,
    'overwrite_output_dir': True,
    'train_batch_size': 32,
    'num_train_epochs': 5,
    'mlm': False,
}

# Create a GPT-2 model that is fine-tuned on fairy tale data.
general_then_author_specific_fine_tuned_model = LanguageModelingModel('gpt2', 'gpt2', args=train_args)

general_then_author_specific_fine_tuned_model.train_model('general_train.txt', eval_file='general_test.txt')
general_then_author_specific_fine_tuned_model.eval_model('general_test.txt')

general_then_author_specific_fine_tuned_model.train_model('author_specific_train.txt', eval_file='author_specific_test.txt')
general_then_author_specific_fine_tuned_model.eval_model('author_specific_test.txt')

"""Generate some fairy tale text from fine-tuned GPT-2 model."""

from simpletransformers.language_generation import LanguageGenerationModel

# Generate fairy tale on fine-tuned model.
generator = LanguageGenerationModel('gpt2', './outputs', args={'max_length': 256})

# Prompts based on fairy tales!
prompts = [
    "Far underneath the sea, there lives a young princess who has seven sisters.",
    "Once upon a time there was a young girl who was the fairest of them all. Her sisters were jealous of her beauty.",
    "Alone in the woods was a small girl in a red cape. She was on her way to bring bread to her grandmother.",
    "A long time ago, there was a couple who wanted more than anything to have a child of their own.",
    "Out in a cave, there lived a horrible beast. Nobody dared go near it, and the cave remained untouched by the nearby village for thousands of years.",
]

for prompt in prompts:
    generated = generator.generate(prompt, verbose=False)

    generated = '.\n'.join(generated[0].split('.')[:-1]) + '.'
    print('\n\n' + generated)