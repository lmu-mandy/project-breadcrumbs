# -*- coding: utf-8 -*-
"""Maddie Louis - Fairytale Perplexity

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zgn50SGX7-QPlzd0Aq048kl37mTnjkDC

This notebook uses GPT-2 from simpletransformers to generate text. First, we use a pre-trained model. Then, we fine-tune GPT-2 on research paper abstracts from arXiv.

Reference: https://medium.com/swlh/learning-to-write-language-generation-with-gpt-2-2a13fa249024

Be sure to use a GPU for this demo! (Edit -> Notebook Settings)
"""

# The lines below only need to be uncommented and run once.
# You may see some errors. Click the button "Restart Runtime" when finished.

!pip install simpletransformers
!wget https://github.com/staeiou/arxiv_archive/raw/master/processed_data/20200101/per_category/cs.AI.tsv.xz
!unxz cs.AI.tsv.xz

import torch
# If there's a GPU available...
if torch.cuda.is_available():    
    # Tell PyTorch to use the GPU.    
    device = torch.device('cuda')
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device('cpu')

"""Prepare the data"""

import pandas as pd

lines = pd.read_csv('bfb.txt', sep=r'\s{2,}', engine='python', header=None, names=['Line'])

stop = len(lines) * 0.8

# df = pd.read_csv('cs.AI.tsv', sep='\t')
# abstracts = df['abstract'].tolist()

i = 0
# Train it on the old, unrelated data
with open('train.txt', 'w') as f:
   for index, row in lines.iterrows():
     if i < stop:
       f.writelines(line + '\n')
     i = i + 1
    # for abstract in abstracts:
    #     f.writelines(abstract + '\n')

# Test it on the fairy tale data
i = 0
with open('test.txt', 'w') as f:
   for index, row in lines.iterrows():
     if i > stop:
       f.writelines(line + '\n')
     i = i + 1

"""Language generation using a pre-trained GPT-2 model


"""

import logging
from simpletransformers.language_generation import LanguageGenerationModel


logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

generation_model = LanguageGenerationModel('gpt2', 'gpt2', args={'max_length': 256})

prompts = [
    "Once upon a time there was a young girl who was the fairest of them all. Her sisters were jealous of her beauty.",
    "Alone in the woods was a small girl in a red cape. She was on her way to bring bread to her grandmother.",
    "A long time ago, there was a couple who wanted more than anything to have a child of their own.",
    "Out in a cave, there lived a horrible beast. Nobody dared go near it, and the cave remained untouched by the nearby village for thousands of years.",
]

for prompt in prompts:
    # Generate text using the model. Verbose set to False to prevent logging generated sequences.
    generated = generation_model.generate(prompt, verbose=False)

    generated = '.'.join(generated[0].split('.')[:-1]) + '.'
    print('\n\n' + generated)

"""Fine-tuning a pre-trained GPT-2 model for domain-specific language generation"""

from simpletransformers.language_modeling import LanguageModelingModel
import logging


logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

# If you get a CUDA memory error, try decreasing the batch size!
train_args = {
    'reprocess_input_data': True,
    'overwrite_output_dir': True,
    'train_batch_size': 32,
    'num_train_epochs': 1,
    'mlm': False,
}

model = LanguageModelingModel('gpt2', 'gpt2', args=train_args, generator_name="outputs/generator_model",)

# model.train_model('train.txt', eval_file='test.txt')

model.eval_model('test.txt')

# TODO: Use the same prompts as before to generate text and compare!

model_2 = LanguageGenerationModel('gpt2', './outputs', args={'max_length': 256})

prompts = [
    "Once upon a time there was a young girl who was the fairest of them all. Her sisters were jealous of her beauty.",
    "Alone in the woods was a small girl in a red cape. She was on her way to bring bread to her grandmother.",
    "A long time ago, there was a couple who wanted more than anything to have a child of their own.",
    "Out in a cave, there lived a horrible beast. Nobody dared go near it, and the cave remained untouched by the nearby village for thousands of years.",
]

for prompt in prompts:
    # Generate text using the model. Verbose set to False to prevent logging generated sequences.
    generated = model_2.generate(prompt, verbose=False)

    generated = '.'.join(generated[0].split('.')[:-1]) + '.'
    print('\n\n' + generated)