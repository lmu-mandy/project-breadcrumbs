# -*- coding: utf-8 -*-
"""Maddie Louis - Fairytale Perplexity

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zgn50SGX7-QPlzd0Aq048kl37mTnjkDC

This notebook uses GPT-2 from simpletransformers to generate fairy tale text. For the baseline, it is not fine-tuned and then tested on fairy tale data from the Blue Fairy Book.
"""

!pip install simpletransformers

import torch
# If there's a GPU available...
if torch.cuda.is_available():    
    # Tell PyTorch to use the GPU.    
    device = torch.device('cuda')
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device('cpu')

"""Prepare the data"""

import pandas as pd

lines = pd.read_csv('bfb.txt', sep=r'\s{2,}', engine='python', header=None, names=['Line'])

# TODO: Find a better way to split up the data 80/20
stop = len(lines) * 0.8

i = 0
# Train data is the first 80% of the fairy tale data
with open('train.txt', 'w') as f:
   for index, line in lines.iterrows():
     if i < stop:
       f.writelines(line + '\n')
     i = i + 1

# Testing data is the last 20% of the fairy tale data
i = 0
with open('test.txt', 'w') as f:
   for index, line in lines.iterrows():
     if i > stop:
       f.writelines(line + '\n')
     i = i + 1

"""Fine-tuning a pre-trained GPT-2 model for domain-specific language generation"""

from simpletransformers.language_modeling import LanguageModelingModel
import logging


logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

# If you get a CUDA memory error, try decreasing the batch size!
train_args = {
    'reprocess_input_data': True,
    'overwrite_output_dir': True,
    'train_batch_size': 32,
    'num_train_epochs': 1,
    'mlm': False,
}

# This model is passed into the GenerationModel in order to produce fairy tale text.
model = LanguageModelingModel('gpt2', 'gpt2', args=train_args)

# Comment out the line below in order to not train the model on the fairy tale data.
# model.train_model('train.txt', eval_file='test.txt')

model.eval_model('test.txt')

from simpletransformers.language_generation import LanguageGenerationModel

model_2 = LanguageGenerationModel('gpt2', './outputs', args={'max_length': 256})

# Prompts based on fairy tales!
prompts = [
    "Once upon a time there was a young girl who was the fairest of them all. Her sisters were jealous of her beauty.",
    "Alone in the woods was a small girl in a red cape. She was on her way to bring bread to her grandmother.",
    "A long time ago, there was a couple who wanted more than anything to have a child of their own.",
    "Out in a cave, there lived a horrible beast. Nobody dared go near it, and the cave remained untouched by the nearby village for thousands of years.",
]

for prompt in prompts:
    generated = model_2.generate(prompt, verbose=False)

    generated = '.'.join(generated[0].split('.')[:-1]) + '.'
    print('\n\n' + generated)