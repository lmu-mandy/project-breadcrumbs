## Project Breadcrumbs Milestone Report
### Maddie Louis, Merissa Tan, and Andrew Arteaga

[CoLab Link](https://colab.research.google.com/drive/1P7IABjyyH1v660COZv5L7Ha7Vwn5lmoD)

Our goal for this project is to create a text generation model that will auto-generate fairy tale texts. Currently, our GPT-2 model generates new and plausible fairy tales after training on large collections of fairy tales from different authors. Even though the fairy tales that the model generates follow a similar syntax or style of writing seen in many fairy tales, sometimes the actual storyline of the fairy tale is confusing or lacks linearity. So, as we work to incorporate a feature that allows users to tell the model to generate fairy tales depending on author-specific features/styles, we hope to fine-tune the model such that the stories that are generated are more plausible and also minimize the perplexity when we check to see if our model’s generated stories match the specific author’s style we are trying to have it mimic.  

Since our baseline and preliminary models are fully functional, we are hoping to achieve similar or better results seen in similar projects. For example, we are hoping to achieve similar results to Michael Mernaugh’s paper on Predicting Short Story Endings. He uses the ROCStories Corpora, containing 100,000 commonplace stories, to determine if clustering can be used to determine the differences in stories and categorize them into types, to allow him to predict which proposed ending of a story was more believable. After using his best hyperparameters, Michael achieved accuracies similar to the accuracies achieved in Mostafazadeh et al’s research, which is around 50% percent accuracy. We are hoping that, by in-class presentations, 50 percent of the sentences generated by our model will be similar to the author’s style that the user chooses.  

Secondly, we are hoping to achieve similar results to a project that students from MIT completed where they decided to determine the difference between canonical and noncanonical fanfiction in their paper Discriminating between canonical from non-canonical fan fiction. They used a SVM (Support Vector Machine) and stemmed bigrams as baseline to look at the fanfiction surrounding J.K. Rowling’s Harry Potter series. Their model was able to accurately determine the difference between canonical and non-canonical fanfiction around 60-70 percent of the time. This means that when our group attempts to analyze the similarities and differences between the structure of our model’s stories and the works of fairy tale authors, we aim to make our model’s generation extremely similar to an actual author’s works such that it is difficult to discern the difference between the two.  

Since our end goal at this time was to create a simple preliminary model that generates new and plausible fairy tales, our group decided to investigate which pre-existing language generation models fulfills most of our needs. While we were each playing around with different language generation models we found on the internet, we discovered, ironically, the GPT-2 baseline we worked with in class seemed to be the perfect model that met all the criteria we were looking for: the ability to generate new and plausible text depending on the data the model is fine-tuned on and determine perplexity given a test set.  

In order to parse the various fairy tale datasets (listed in the table below), we downloaded the data as text files and pre-processed them by manually removing the headers and footers that contained metadata about the datasets. Then, we used pandas to parse the data into an array of fairy tale texts. For the purposes of defining our baseline and preliminary models, we divided the dataset into 80% training data and 20% test data. We did not assign a portion of the dataset for validation, since we are not fine-tuning the models to improve their accuracies. Instead, we compared perplexity numbers of both the baseline and fine-tuned models.  

For our milestone results, we calculated the perplexity of both the GPT-2 baseline model and fine-tuned preliminary model on several different fairy tale datasets. All of the results from these findings are shown in the table below. The baseline model was not fine-tuned on any data, but was tested on each of the different fairy tale text files. The perplexities for this baseline ranged from around 40-60, with the highest being for the Hans Christian Andersen data and the lowest for the Blue Fairy Book data. We used multiple datasets in order to be able to examine the differences in generated text as well as perplexity, which will help us in the future when we aggregate datasets and divide them based on authors.  

The fine-tuned model was both trained and tested on each of the datasets, with the first 80% of the data used for training and the remaining 20% used for testing. After training the model with 5 epochs, the perplexities for the fine-tuned models ranged from about 30-50, which is significantly lower than the perplexities for the baseline. This trend follows what we expected, since a model trained on fairy tale data should be less perplexed when tested on similar data, as opposed to an untrained model. Additionally, the table includes selected samples from text generated by the fine-tuned fairy tale model.  

In order to achieve our milestone and obtain these preliminary results, we worked separately with different datasets to fine-tune the model and calculate its perplexity, then met together to combine our work and determine which methods we had come up with were best. From there, we did pair programming and collaborated on the final version of the baseline and fine-tuned models. Therefore, we did not use separate branches, since we formed the final product together.  

| Test Dataset Used                            | Baseline Perplexity | Fine-tuned Perplexity | Fine-tuned Generation Sample                                                                                                                                                                                                                                                                                                                                                                                       |
|----------------------------------------------|---------------------|-----------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [The Blue Fairy Book](http://www.gutenberg.org/files/503/503-h/503-h.htm)<br>(Mixed Authors)       | 41.0371             | 29.3047               | The Prince took her in his arms and kissed her gently; <br>but when he came to the Fairy he thought he would have the <br>very most pleasant thing to do. In spite of all the fuss <br>he found his way to the Fairy's house. But there was nothing<br>there except a long staircase, which was covered with flowers.                                                                                              |
| [Hans Christian Andersen<br>Tales](http://www.gutenberg.org/ebooks/27200)             | 62.7144             | 43.4158               | And he said to him, "Look at the little boy! He looks like a lion;<br>if you can look at it, your daughter will love him as she loves<br>her brother." That was a very unhappy expression, for, indeed,<br>every time one spoke to him, there was a change in his complexion;<br>the tears of happiness came again in the little brother.                                                                          |
| [Kaggle Fairy Tale Dataset](https://www.kaggle.com/cuddlefish/fairy-tales/activity)<br>(Mixed Authors) | 60.0117             | 45.2276               | But what they could not tell the king, and how his sister had<br>betrayed them, the princesses and the princes felt compelled to<br>tell the old lady that they had no idea what had happened, and<br>what to do with the prince who had promised their daughter to a<br>beautiful woman. It was not till the queen's daughter had taken<br>them into a great palace that they felt free to marry them themselves. |