NLP Final Project Proposal: Breadcrumbs  
Team: Maddie Louis, Merissa Tan, Andrew Arteaga (MMA)  

  Our goal for this project is to create a text generation model that will auto-generate fairy tale texts. The minimum functionality of this model will generate
a new and reasonable fairy tale after the model trains on a large collection of existing fairy tales. We believe that we will be able to achieve this goal;
however, the fairy tale that is generated might not be perfect in the aspect of storytelling. We are also hoping to incorporate author-specific features and use
the model to generate fairy tales based on the writing styles of famous fairy tale authors such as the Brothers Grimm and Charles Perrault. We will then analyze
the accuracy of the model and consider the similarities and differences between the texts that it generates for different authors.  
	For our project, we are planning on utilizing a few different datasets. Project Gutenberg provides access to many different original fairy tales, and there are also a few fairy tale datasets available on Kaggle. Since we are planning on generating text in the style of different authors, we will most likely use a
combination of both data from Kaggle and Project Gutenberg. We will use a GPT-2 model in order to generate the text, and train it specifically on the fairy tale data. For a baseline comparison and for overall evaluation, we will use perplexity to determine how well the model is doing. For the baseline, we will examine the perplexity of randomized output from the "bag of words" feature, using tokens extracted from the datasets. This baseline comparison will give us a general understanding of how well our model is doing and how intelligent it is. In addition to examining how the model's perplexity score changes as it is tweaked, our team will eventually manually examine output and consider its reasonability.

<b>Preliminary schedule:</b>   
Week 10: Finalize proposal and complete research for suitable datasets and possible models (all)   
Week 11: Begin building model (Maddie, Merissa) and figure out how to consolidate and import fairy tale datasets into model in order to begin training (Andrew)  
Week 12: Start training the model and analyze training loss (all)  
Week 13: Begin generating fairy tale text and evaluate perplexity (all), consider whether the generated text is appropriate (Maddie, Merissa), complete milestone report with preliminary results (all)  
Week 14: Tweak and fine-tune baseline model to improve accuracy (Andrew), begin working on features for author-based generation (Merissa, Maddie)  
Week 15: Work on project presentations, evaluate author-based text generation results (all)  
Week 16: Finalize model, complete final report (all)  
  
  Similar projects have been done that look at predicting the ending of stories based on the story type and discriminating between canonical and non-canonical fan
fiction. First, Michael Mernaugh’s paper on Predicting Short Story Endings uses the ROCStories Corpora, containing 100,000 commonplace stories, to determine if
clustering can be used to determine the differences in stories and categorize them into types, to allow him to predict which proposed ending of a story was more
believable. The stories he used for training were 5 sentences long and the stories used for validation and testing were 4 sentences long and had 2 possible
endings. Michael’s goal was to achieve better accuracies than Mostafazadeh et als’, the Story Cloze Benchmark that also attempts to accurately predict the ending
of commonplace stories,  accuracies, but the accuracies Michael achieved after using his best hyperparameters shows that the accuracies are similar to the
accuracies achieved in Mostafazadeh et al’s research, which is around 50% percent accuracy. Both Michael’s and Mostafazadeh et al’s research tells us that when our
group attempts to generate stories based on the author’s style, that our goal is to have 50 percent of the sentences generated by our model be similar to the
author’s style.  
	Second, students from MIT decided to determine the difference between canonical and noncanonical fanfiction in their paper Discriminating between canonical
from non-canonical fan fiction. They used a SVM(Support Vector Machine) and stemmed bigrams as baseline to look at the fanfiction surrounding J.K. Rowling’s
Harry Potter series. Most canonical fanfictions followed a structured pattern and specific detailed description when describing certain events, so after using
the data from the Harry Potter fanfiction archive (http://www.harrypotterfanfiction.com/), their model was able to accurately determine the difference between
canonical and non-canonical fanfiction around 60-70 percent of the time. This means that when our group attempts to analyze the similarities and differences
between the structure of our model’s stories and the works of fairy tale authors, our goal is to make our model be able to discern the differences between
canonical, actual authors, works vs non-canonical, our model’s, works around 60 to 70 percent of the time. 





<b>Works Referenced</b>  
  
Mernagh, Michael. Predicting Short Story Endings. web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/reports/2735427.pdf.  
  
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv preprint arXiv:1604.01696, 2016.  
  
“Past 6.806/6.864 Projects.” Courses.csail.mit.edu, courses.csail.mit.edu/6.864/fall_2016/past_projects.html.  


